{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a203d5b",
   "metadata": {},
   "source": [
    "# Task 2 Baseline Notebook: “Adult Census Income” Classification Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fcd50b",
   "metadata": {},
   "source": [
    "## 1. Problem Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63349922",
   "metadata": {},
   "source": [
    "The goal of this project is to build a classification model that, given a person’s demographic and socioeconomic attributes, predicts whether their annual income is **greater than** or **less than or equal to** \\$50,000. This is a classic binary supervised learning task illustrating the entire data science workflow: data loading and cleaning, preprocessing and feature engineering, model training, evaluation, and interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e890d7d1",
   "metadata": {},
   "source": [
    "## 2. Dataset (“Adult”)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77466a4b",
   "metadata": {},
   "source": [
    "- **Source**: Extracted from the 1994 U.S. Census by Barry Becker and Ron Kohavi, donated to the UCI Machine Learning Repository.  \n",
    "- **Size**:  \n",
    "  - Total: 48,842 records (32,561 training, 16,281 evaluation)  \n",
    "  - After removing missing values: 45,222 (30,162 train, 15,060 evaluation)  \n",
    "- **Instances**: Individuals over 16 years old with gross income > \\$100 and working > 0 hours/week.  \n",
    "- **Attributes**: 14 features (6 continuous, 8 categorical) + 1 target variable.  \n",
    "- **Target variable** (`income`):  \n",
    "  - `>50K` (annual income over \\$50,000)  \n",
    "  - `<=50K` (annual income \\$50,000 or less)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fb8e68",
   "metadata": {},
   "source": [
    "### 2.1 Continuous Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0108f324",
   "metadata": {},
   "source": [
    "- `age`: Age in years  \n",
    "- `fnlwgt`: Final weight (population expansion factor)  \n",
    "- `education_num`: Number of years of formal education  \n",
    "- `capital_gain`: Non‐recurring capital gains (USD)  \n",
    "- `capital_loss`: Non‐recurring capital losses (USD)  \n",
    "- `hours_per_week`: Hours worked per week"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adad8e0c",
   "metadata": {},
   "source": [
    "### 2.2 Categorical Attributes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65aad26d",
   "metadata": {},
   "source": [
    "- `workclass`, `education`, `marital_status`, `occupation`, `relationship`, `race`, `sex`, `native_country` (various categories as documented)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c54828e",
   "metadata": {},
   "source": [
    "## 3. Workflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd88d753",
   "metadata": {},
   "source": [
    "1. **Load & Explore**  \n",
    "2. **Clean Data**  \n",
    "3. **Preprocess** (encode categoricals, impute missing)  \n",
    "4. **Feature Engineering**  \n",
    "5. **Train/Test Split**  \n",
    "6. **Modeling** (Random Forest, Gradient Boosting, etc.)  \n",
    "7. **Evaluation** (Accuracy, AUC, custom Score)  \n",
    "8. **Submit**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1960221c",
   "metadata": {},
   "source": [
    "## 4. Prediction Objective\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f644da18",
   "metadata": {},
   "source": [
    "Predict `income` (`>50K` vs. `<=50K`) as accurately as possible, demonstrating skills in data cleaning, modeling, and evaluation of real‐world datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3258bff",
   "metadata": {},
   "source": [
    "## 5. Baseline solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e934c5",
   "metadata": {},
   "source": [
    "### 5.1 - Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5be39ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9188fa38",
   "metadata": {},
   "source": [
    "### Importance of Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711c647c",
   "metadata": {},
   "source": [
    "A machine learning model’s quality depends on the data’s quality. Dirty or incomplete data can:\n",
    "- **Bias** estimates if certain groups are underrepresented.\n",
    "- **Reduce** useful data volume by dropping too many rows.\n",
    "- **Cause errors** or unreliable training results.\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. Dropping Missing Rows\n",
    "\n",
    "The simplest approach is to remove any row with `NaN`:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('adult.csv')\n",
    "df_clean = df.dropna()\n",
    "# Warning: Removing many rows may waste a lot of valuable data and may bring inconsistencies with evaluation data.\n",
    "```\n",
    "\n",
    "#### 2. Imputation with Central Tendency\n",
    "\n",
    "Rather than deleting rows, replace `NaN` with statistical values:\n",
    "\n",
    "- **Numeric → Median** (robust to outliers):\n",
    "\n",
    "  ```python\n",
    "  import numpy as np\n",
    "\n",
    "  num_cols = df.select_dtypes(include=[np.number]).columns\n",
    "  for col in num_cols:\n",
    "      med = df[col].median()\n",
    "      df[col] = df[col].fillna(med)\n",
    "  ```\n",
    "\n",
    "- **Categorical → Mode** (most frequent category):\n",
    "\n",
    "  ```python\n",
    "  cat_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "  for col in cat_cols:\n",
    "      mode = df[col].mode()[0]\n",
    "      df[col] = df[col].fillna(mode)\n",
    "  ```\n",
    "\n",
    "**Benefits of Imputation**\n",
    "- Retains most of your data.\n",
    "- Reduces bias from dropping entire rows.\n",
    "- Easy to implement in pandas with a few lines.\n",
    "\n",
    "Next, explore advanced methods (KNN Imputer, MICE, model-based) once you’re comfortable with these basics.  \n",
    "See more: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.fillna.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321b8b69",
   "metadata": {},
   "source": [
    "### 5.2 - Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f142d1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train data (set the path to the CSV file)\n",
    "df_train_full = pd.read_csv('train_task2.csv', na_values='?', skipinitialspace=True)\n",
    "\n",
    "# Load evaluation data (set the path to the CSV file)\n",
    "df_eval = pd.read_csv('eval_task2.csv', na_values='?', skipinitialspace=True)\n",
    "\n",
    "# Store ids\n",
    "eval_ids = df_eval['id']\n",
    "# Remove id column\n",
    "df_eval = df_eval.drop('id', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bc8e65",
   "metadata": {},
   "source": [
    "### 5.3 - Count missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c56bec3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in train data:\n",
      " workclass         1836\n",
      "occupation        1843\n",
      "native_country     583\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check missing values in train data\n",
    "missing_train = df_train_full.isnull().sum()\n",
    "print(\"Missing values in train data:\\n\", missing_train[missing_train > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80176eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in evaluation data:\n",
      " workclass         963\n",
      "occupation        966\n",
      "native_country    274\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check missing values in evaluation data\n",
    "missing_eval = df_eval.isnull().sum()\n",
    "print(\"Missing values in evaluation data:\\n\", missing_eval[missing_eval > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cb296f",
   "metadata": {},
   "source": [
    "### 5.4 - Impute training and evaluation data with median/mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40d3fd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify your columns\n",
    "numeric_cols     = df_train_full.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = df_train_full.select_dtypes(include=['object','category']).columns.tolist()\n",
    "# If 'income' is categorical, remove it from features\n",
    "if 'income' in categorical_cols:\n",
    "    categorical_cols.remove('income')\n",
    "\n",
    "# Impute missing values\n",
    "# For numeric columns, use median; for categorical, use mode\n",
    "num_cols = df_train_full.select_dtypes(include=[np.number]).columns\n",
    "for col in numeric_cols:\n",
    "    med_train = df_train_full[col].median()\n",
    "    med_eval = df_eval[col].median()\n",
    "\n",
    "    df_train_full[col] = df_train_full[col].fillna(med_train)\n",
    "    df_eval[col] = df_eval[col].fillna(med_eval)\n",
    "\n",
    "cat_cols = df_train_full.select_dtypes(include=['object', 'category']).columns\n",
    "for col in categorical_cols:\n",
    "    mode_train = df_train_full[col].mode()[0]\n",
    "    mode_eval = df_eval[col].mode()[0]\n",
    "    \n",
    "    df_train_full[col] = df_train_full[col].fillna(mode_train)\n",
    "    df_eval[col] = df_eval[col].fillna(mode_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a30a40",
   "metadata": {},
   "source": [
    "### 5.5 - Check no missing values left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e04b27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing values left in train and evaluation data.\n"
     ]
    }
   ],
   "source": [
    "# Check there are no missing values left\n",
    "missing_train = df_train_full.isnull().sum()\n",
    "missing_eval = df_eval.isnull().sum()\n",
    "if missing_train.sum() == 0 and missing_eval.sum() == 0:\n",
    "    print(\"No missing values left in train and evaluation data.\")\n",
    "else:\n",
    "    print(\"There are still missing values in the data.\")\n",
    "    print(\"Train data missing values:\\n\", missing_train[missing_train > 0])\n",
    "    print(\"Evaluation data missing values:\\n\", missing_eval[missing_eval > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae3588a",
   "metadata": {},
   "source": [
    "### Handling Numeric and Categorical Features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb161023",
   "metadata": {},
   "source": [
    "\n",
    "Real-world tables often mix:\n",
    "\n",
    "- **Numeric features** (e.g., age, salary, hours worked)  \n",
    "- **Categorical features** (e.g., gender, marital status, country)\n",
    "\n",
    "Most ML algorithms require **numeric input**, so we must encode categories without imposing false order or exploding dimensions.\n",
    "\n",
    "#### Common Encoding Techniques\n",
    "\n",
    "1. **Label Encoding**  \n",
    "   - Maps each category to a unique integer.  \n",
    "   - *Pros:* Simple and fast.  \n",
    "   - *Cons:* Imposes arbitrary order.\n",
    "\n",
    "   ```python\n",
    "   from sklearn.preprocessing import LabelEncoder\n",
    "   le = LabelEncoder()\n",
    "   df['color_enc'] = le.fit_transform(df['color'])\n",
    "   ```\n",
    "\n",
    "2. **One-Hot Encoding**  \n",
    "   - Creates one binary column per category.  \n",
    "   - *Pros:* No order.  \n",
    "   - *Cons:* High dimensionality if many categories.\n",
    "\n",
    "   ```python\n",
    "   df = pd.get_dummies(df, columns=['color'])\n",
    "   ```\n",
    "\n",
    "3. **Ordinal Encoding**  \n",
    "   - Uses integers for naturally ordered categories (e.g., education levels).  \n",
    "   - *Example:* `Preschool`→0, `HS-grad`→3, `Bachelors`→5, `Masters`→6.\n",
    "\n",
    "4. **Mean/Target Encoding**  \n",
    "   - Replaces each category with the average target value for that category.  \n",
    "   - Useful for high‑cardinality features, but watch for overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3ae122",
   "metadata": {},
   "source": [
    "### 5.6 - Enconde categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f78d877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categoricals\n",
    "le = LabelEncoder()\n",
    "for col in categorical_cols:\n",
    "    df_train_full[col] = le.fit_transform(df_train_full[col])\n",
    "    df_eval[col] = le.transform(df_eval[col])\n",
    "\n",
    "# Encode 'income' in the train set\n",
    "df_train_full['income'] = le.fit_transform(df_train_full['income'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18ae591",
   "metadata": {},
   "source": [
    "### Why Feature Scaling Matters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3377fa39",
   "metadata": {},
   "source": [
    "\n",
    "Numeric features often differ widely in range (e.g., age 17–90 vs. population weight tens to hundreds of thousands).  \n",
    "Models that rely on distances or gradients (linear regression, SVM, neural nets, KNN) can be skewed when one feature dominates:\n",
    "\n",
    "- **Domination**: large-range features drive the gradient or distance.  \n",
    "- **Slow or unstable convergence**: gradient steps optimal for one dimension may be too big or small for another.  \n",
    "- **Uneven regularization**: L1/L2 penalties depend on feature scale.\n",
    "\n",
    "#### When to Scale\n",
    "- **Scale-sensitive models**: linear/logistic regression, SVM, k‑NN, neural networks, PCA  \n",
    "- **Scale-insensitive**: tree-based models (decision trees, random forest, boosting)\n",
    "\n",
    "#### Common Scaling Methods\n",
    "1. **Min–Max**: map to [0–1] or [1–100]  \n",
    "2. **Standard (Z-score)**: zero mean, unit variance  \n",
    "3. **Robust**: uses median and IQR to resist outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519c983c",
   "metadata": {},
   "source": [
    "### 5.7 - Scale numerics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76e210ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale numerics\n",
    "scaler = MinMaxScaler(feature_range=(1, 100))\n",
    "df_train_full.loc[:, numeric_cols] = scaler.fit_transform(df_train_full[numeric_cols])\n",
    "df_eval.loc[:, numeric_cols] = scaler.transform(df_eval[numeric_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51db201",
   "metadata": {},
   "source": [
    "### Model Choice and Alternatives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1115cc",
   "metadata": {},
   "source": [
    "\n",
    "While **Logistic Regression** is simple and interpretable, it’s not always best. Consider:\n",
    "\n",
    "- **Logistic Regression**  \n",
    "  - *Pros:* Fast, interpretable coefficients, natural for binary labels.  \n",
    "  - *Cons:* Assumes linear decision boundary, needs manual feature interactions.\n",
    "\n",
    "- **Decision Trees**  \n",
    "  - *Pros:* Captures nonlinearity and interactions automatically.  \n",
    "  - *Cons:* Prone to overfitting without pruning.\n",
    "\n",
    "- **Ensembles**  \n",
    "  - **Random Forest**: robust, handles missing data, minimal tuning.  \n",
    "  - **Gradient Boosting** (LightGBM, XGBoost): state-of-the-art for tabular data.\n",
    "\n",
    "- **SVM**: Effective with kernels, needs scaling, may be slow for large data.  \n",
    "- **KNN**: Simple neighbor-based, sensitive to scale and data density.  \n",
    "- **Naive Bayes**: Fast, good with categoricals, assumes feature independence.  \n",
    "- **Neural Networks**: Flexible but require more data, tuning, compute."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349e2241",
   "metadata": {},
   "source": [
    "### 5.8 - Logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e60e2513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1: 0.562\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Separate features (X) and response variable (y)\n",
    "X = df_train_full.drop('income', axis=1)\n",
    "y = df_train_full['income']\n",
    "\n",
    "# Divide into training (80%) and testing (20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    train_size=0.8,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# Create and train the model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "acc    = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Test F1: {acc:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375af54b",
   "metadata": {},
   "source": [
    "### 5.9 - Train your model again with all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e57ca450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059c555b",
   "metadata": {},
   "source": [
    "### 5.10 - Make predictions on the evaluation data and save to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b3b3466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to predictions.csv!\n"
     ]
    }
   ],
   "source": [
    "# Predict using the trained model\n",
    "y_eval_pred_encoded = model.predict(df_eval)\n",
    "\n",
    "# Decode the predictions back to original labels\n",
    "y_eval_pred = le.inverse_transform(y_eval_pred_encoded)\n",
    "\n",
    "# Combine the IDs and predictions into a DataFrame\n",
    "df_submission = pd.DataFrame({\n",
    "    'id': eval_ids,\n",
    "    'prediction': y_eval_pred\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_submission.to_csv('predictions_task2.csv', index=False)\n",
    "\n",
    "print(\"Predictions saved to predictions.csv!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93143aed",
   "metadata": {},
   "source": [
    "## 6. New solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e217a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
