{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63a533c0",
   "metadata": {},
   "source": [
    "# Task 2 Baseline Notebook: “Adult Census Income” Classification Project\n",
    "\n",
    "## 1. Problem Description\n",
    "The goal of this project is to build a classification model that, given a person’s demographic and socioeconomic attributes, predicts whether their annual income is **greater than** or **less than or equal to** \\$50,000. This is a classic binary supervised learning task illustrating the entire data science workflow: data loading and cleaning, preprocessing and feature engineering, model training, evaluation, and interpretation.\n",
    "\n",
    "## 2. Dataset (“Adult”)\n",
    "\n",
    "- **Source**: Extracted from the 1994 U.S. Census by Barry Becker and Ron Kohavi, donated to the UCI Machine Learning Repository.  \n",
    "- **Size**:  \n",
    "  - Total: 48,842 records (32,561 training, 16,281 evaluation)  \n",
    "  - After removing missing values: 45,222 (30,162 train, 15,060 evaluation)  \n",
    "- **Instances**: Individuals over 16 years old with gross income > \\$100 and working > 0 hours/week.  \n",
    "- **Attributes**: 14 features (6 continuous, 8 categorical) + 1 target variable.  \n",
    "- **Target variable** (`income`):  \n",
    "  - `>50K` (annual income over \\$50,000)  \n",
    "  - `<=50K` (annual income \\$50,000 or less)\n",
    "\n",
    "### 2.1 Continuous Attributes\n",
    "- `age`: Age in years  \n",
    "- `fnlwgt`: Final weight (population expansion factor)  \n",
    "- `education_num`: Number of years of formal education  \n",
    "- `capital_gain`: Non‐recurring capital gains (USD)  \n",
    "- `capital_loss`: Non‐recurring capital losses (USD)  \n",
    "- `hours_per_week`: Hours worked per week\n",
    "\n",
    "### 2.2 Categorical Attributes\n",
    "- `workclass`, `education`, `marital_status`, `occupation`, `relationship`, `race`, `sex`, `native_country` (various categories as documented)\n",
    "\n",
    "## 3. Workflow\n",
    "\n",
    "1. **Load & Explore**  \n",
    "2. **Clean Data**  \n",
    "3. **Preprocess** (encode categoricals, impute missing)  \n",
    "4. **Feature Engineering**  \n",
    "5. **Train/Test Split**  \n",
    "6. **Modeling** (Random Forest, Gradient Boosting, etc.)  \n",
    "7. **Evaluation** (Accuracy, AUC, custom Score)  \n",
    "8. **Submit**\n",
    "\n",
    "## 4. Prediction Objective\n",
    "Predict `income` (`>50K` vs. `<=50K`) as accurately as possible, demonstrating skills in data cleaning, modeling, and evaluation of real‐world datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3258bff",
   "metadata": {},
   "source": [
    "# Possible solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be39ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711c647c",
   "metadata": {},
   "source": [
    "## Importance of Data Cleaning\n",
    "\n",
    "A machine learning model’s quality depends on the data’s quality. Dirty or incomplete data can:\n",
    "- **Bias** estimates if certain groups are underrepresented.\n",
    "- **Reduce** useful data volume by dropping too many rows.\n",
    "- **Cause errors** or unreliable training results.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Dropping Missing Rows\n",
    "\n",
    "The simplest approach is to remove any row with `NaN`:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('adult.csv')\n",
    "df_clean = df.dropna()\n",
    "# Warning: Removing many rows may waste a lot of valuable data and may bring inconsistencies with evaluation data.\n",
    "```\n",
    "\n",
    "### 2. Imputation with Central Tendency\n",
    "\n",
    "Rather than deleting rows, replace `NaN` with statistical values:\n",
    "\n",
    "- **Numeric → Median** (robust to outliers):\n",
    "\n",
    "  ```python\n",
    "  import numpy as np\n",
    "\n",
    "  num_cols = df.select_dtypes(include=[np.number]).columns\n",
    "  for col in num_cols:\n",
    "      med = df[col].median()\n",
    "      df[col] = df[col].fillna(med)\n",
    "  ```\n",
    "\n",
    "- **Categorical → Mode** (most frequent category):\n",
    "\n",
    "  ```python\n",
    "  cat_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "  for col in cat_cols:\n",
    "      mode = df[col].mode()[0]\n",
    "      df[col] = df[col].fillna(mode)\n",
    "  ```\n",
    "\n",
    "**Benefits of Imputation**\n",
    "- Retains most of your data.\n",
    "- Reduces bias from dropping entire rows.\n",
    "- Easy to implement in pandas with a few lines.\n",
    "\n",
    "Next, explore advanced methods (KNN Imputer, MICE, model-based) once you’re comfortable with these basics.  \n",
    "See more: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.fillna.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f142d1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train data (set the path to the CSV file)\n",
    "df_train_full = pd.read_csv('train_task2.csv', na_values='?', skipinitialspace=True)\n",
    "\n",
    "# Load evaluation data (set the path to the CSV file)\n",
    "df_eval = pd.read_csv('eval_task2.csv', na_values='?', skipinitialspace=True)\n",
    "\n",
    "# Store ids\n",
    "eval_ids = df_eval['id']\n",
    "# Remove id column\n",
    "df_eval = df_eval.drop('id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56bec3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values in train data\n",
    "missing_train = df_train_full.isnull().sum()\n",
    "print(\"Missing values in train data:\\n\", missing_train[missing_train > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80176eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values in evaluation data\n",
    "missing_eval = df_eval.isnull().sum()\n",
    "print(\"Missing values in evaluation data:\\n\", missing_eval[missing_eval > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d3fd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify your columns\n",
    "numeric_cols     = df_train_full.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = df_train_full.select_dtypes(include=['object','category']).columns.tolist()\n",
    "# If 'income' is categorical, remove it from features\n",
    "if 'income' in categorical_cols:\n",
    "    categorical_cols.remove('income')\n",
    "\n",
    "# Impute missing values\n",
    "# For numeric columns, use median; for categorical, use mode\n",
    "num_cols = df_train_full.select_dtypes(include=[np.number]).columns\n",
    "for col in numeric_cols:\n",
    "    med_train = df_train_full[col].median()\n",
    "    med_eval = df_eval[col].median()\n",
    "\n",
    "    df_train_full[col] = df_train_full[col].fillna(med_train)\n",
    "    df_eval[col] = df_eval[col].fillna(med_eval)\n",
    "\n",
    "cat_cols = df_train_full.select_dtypes(include=['object', 'category']).columns\n",
    "for col in categorical_cols:\n",
    "    mode_train = df_train_full[col].mode()[0]\n",
    "    mode_eval = df_eval[col].mode()[0]\n",
    "    \n",
    "    df_train_full[col] = df_train_full[col].fillna(mode_train)\n",
    "    df_eval[col] = df_eval[col].fillna(mode_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e04b27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check there are no missing values left\n",
    "missing_train = df_train_full.isnull().sum()\n",
    "missing_eval = df_eval.isnull().sum()\n",
    "if missing_train.sum() == 0 and missing_eval.sum() == 0:\n",
    "    print(\"No missing values left in train and evaluation data.\")\n",
    "else:\n",
    "    print(\"There are still missing values in the data.\")\n",
    "    print(\"Train data missing values:\\n\", missing_train[missing_train > 0])\n",
    "    print(\"Evaluation data missing values:\\n\", missing_eval[missing_eval > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb161023",
   "metadata": {},
   "source": [
    "## Handling Numeric and Categorical Features\n",
    "\n",
    "Real-world tables often mix:\n",
    "\n",
    "- **Numeric features** (e.g., age, salary, hours worked)  \n",
    "- **Categorical features** (e.g., gender, marital status, country)\n",
    "\n",
    "Most ML algorithms require **numeric input**, so we must encode categories without imposing false order or exploding dimensions.\n",
    "\n",
    "### Common Encoding Techniques\n",
    "\n",
    "1. **Label Encoding**  \n",
    "   - Maps each category to a unique integer.  \n",
    "   - *Pros:* Simple and fast.  \n",
    "   - *Cons:* Imposes arbitrary order.\n",
    "\n",
    "   ```python\n",
    "   from sklearn.preprocessing import LabelEncoder\n",
    "   le = LabelEncoder()\n",
    "   df['color_enc'] = le.fit_transform(df['color'])\n",
    "   ```\n",
    "\n",
    "2. **One-Hot Encoding**  \n",
    "   - Creates one binary column per category.  \n",
    "   - *Pros:* No order.  \n",
    "   - *Cons:* High dimensionality if many categories.\n",
    "\n",
    "   ```python\n",
    "   df = pd.get_dummies(df, columns=['color'])\n",
    "   ```\n",
    "\n",
    "3. **Ordinal Encoding**  \n",
    "   - Uses integers for naturally ordered categories (e.g., education levels).  \n",
    "   - *Example:* `Preschool`→0, `HS-grad`→3, `Bachelors`→5, `Masters`→6.\n",
    "\n",
    "4. **Mean/Target Encoding**  \n",
    "   - Replaces each category with the average target value for that category.  \n",
    "   - Useful for high‑cardinality features, but watch for overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f78d877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Encode categoricals\n",
    "le = LabelEncoder()\n",
    "for col in categorical_cols:\n",
    "    df_train_full[col] = le.fit_transform(df_train_full[col])\n",
    "    df_eval[col] = le.transform(df_eval[col])\n",
    "\n",
    "# 2) Encode 'income' in the train set\n",
    "df_train_full['income'] = le.fit_transform(df_train_full['income'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3377fa39",
   "metadata": {},
   "source": [
    "### Why Feature Scaling Matters\n",
    "\n",
    "Numeric features often differ widely in range (e.g., age 17–90 vs. population weight tens to hundreds of thousands).  \n",
    "Models that rely on distances or gradients (linear regression, SVM, neural nets, KNN) can be skewed when one feature dominates:\n",
    "\n",
    "- **Domination**: large-range features drive the gradient or distance.  \n",
    "- **Slow or unstable convergence**: gradient steps optimal for one dimension may be too big or small for another.  \n",
    "- **Uneven regularization**: L1/L2 penalties depend on feature scale.\n",
    "\n",
    "#### When to Scale\n",
    "- **Scale-sensitive models**: linear/logistic regression, SVM, k‑NN, neural networks, PCA  \n",
    "- **Scale-insensitive**: tree-based models (decision trees, random forest, boosting)\n",
    "\n",
    "#### Common Scaling Methods\n",
    "1. **Min–Max**: map to [0–1] or [1–100]  \n",
    "2. **Standard (Z-score)**: zero mean, unit variance  \n",
    "3. **Robust**: uses median and IQR to resist outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e210ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Scale numerics\n",
    "scaler = MinMaxScaler(feature_range=(1, 100))\n",
    "df_train_full.loc[:, numeric_cols] = scaler.fit_transform(df_train_full[numeric_cols])\n",
    "df_eval.loc[:, numeric_cols] = scaler.transform(df_eval[numeric_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1115cc",
   "metadata": {},
   "source": [
    "## Model Choice and Alternatives\n",
    "\n",
    "While **Logistic Regression** is simple and interpretable, it’s not always best. Consider:\n",
    "\n",
    "- **Logistic Regression**  \n",
    "  - *Pros:* Fast, interpretable coefficients, natural for binary labels.  \n",
    "  - *Cons:* Assumes linear decision boundary, needs manual feature interactions.\n",
    "\n",
    "- **Decision Trees**  \n",
    "  - *Pros:* Captures nonlinearity and interactions automatically.  \n",
    "  - *Cons:* Prone to overfitting without pruning.\n",
    "\n",
    "- **Ensembles**  \n",
    "  - **Random Forest**: robust, handles missing data, minimal tuning.  \n",
    "  - **Gradient Boosting** (LightGBM, XGBoost): state-of-the-art for tabular data.\n",
    "\n",
    "- **SVM**: Effective with kernels, needs scaling, may be slow for large data.  \n",
    "- **KNN**: Simple neighbor-based, sensitive to scale and data density.  \n",
    "- **Naive Bayes**: Fast, good with categoricals, assumes feature independence.  \n",
    "- **Neural Networks**: Flexible but require more data, tuning, compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8009f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60e2513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features (X) and response variable (y)\n",
    "X = df_train_full.drop('income', axis=1)\n",
    "y = df_train_full['income']\n",
    "\n",
    "# Divide into training (80%) and testing (20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    train_size=0.8,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# Create and train the model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "acc    = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Test F1: {acc:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375af54b",
   "metadata": {},
   "source": [
    "## Train your model again with all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57ca450",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059c555b",
   "metadata": {},
   "source": [
    "## Make predictions on the evaluation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3b3466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using the trained model\n",
    "y_eval_pred = model.predict(df_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7591fb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the IDs and predictions into a DataFrame\n",
    "df_submission = pd.DataFrame({\n",
    "    'id': eval_ids,\n",
    "    'prediction': y_eval_pred\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d65edc",
   "metadata": {},
   "source": [
    "## Save predictions to a CSV file\n",
    "\n",
    "We create a new DataFrame with the `id` and your model's predictions, then save it.  \n",
    "Make sure your file is named `predictions.csv` for submission!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac71d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "df_submission.to_csv('predictions_task2.csv', index=False)\n",
    "\n",
    "print(\"Predictions saved to predictions.csv!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
